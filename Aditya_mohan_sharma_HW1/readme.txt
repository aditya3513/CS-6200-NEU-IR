how to run:

	1) you will have to install "bs4" for using BeautifulSoup, you can do this by doing pip instal bs4
	
	2) you will have to create folders by the following names [refer to Question 1 or 3 information below to see respective file names for each part]:
		a) task 1 - time zone : html_files_q1_time_zone
		b) task 1 - carbon footprint : html_files_q1_carbon_footprint
		c) task 1 - electric car : html_files_q1_electric_car

	3) Now you can run the file by doing python task1_time_zone.py or python task3.py or python task2.py

	4) it then generates the html files in the respective folders as mentioned below and list of urls in file names as mentioned below. It also print max depth reached, what is the file name and how many uniques url were crawled.

	5) A summarized result is given below.




#############################################

Question 1:
	
	Time zone
	max depth : 3
	number of url visited : 1000
	url list file name : visited_link_file_q1_time_zone.txt
	code file name : task1_time_zone.py

	Electric Car
	max depth : 3
	number of url visited : 1000
	url list file name : visited_link_file_q1_electric_car.txt
	code file name : task1_electric_car.py

	Carbon Footprint
	max depth : 3
	number of url visited : 1000
	url list file name : visited_link_file_q1_carbon_footprint.txt
	code file name : task1_carbon_footprint.py


###############################################

Question 2:
	
	url list file name : merged_file_q2.txt
	code file name : task2.py
	explanation : Task2_explanation.txt

###############################################

Question 3:
	
	max depth : 5
	number of url visited : 1000
	url list file name : visited_link_file_q3.txt
	code file name : task3.py
	explanation : Task3_explanation.txt
	
